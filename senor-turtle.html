<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Señor Turtle - Febin Wilson</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        html {
            scroll-behavior: smooth;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', sans-serif;
            background: #ffffff;
            color: #000000;
        }

        .back-btn {
            position: fixed;
            top: 40px;
            left: 40px;
            color: #000000;
            text-decoration: none;
            font-weight: 500;
            font-size: 1rem;
            z-index: 1000;
            transition: all 0.3s ease;
        }

        .back-btn:hover {
            opacity: 0.7;
        }

        /* Hero Section */
        .hero {
            height: 70vh;
            background: #ffffff;
            display: flex;
            align-items: center;
            justify-content: center;
            text-align: center;
            color: #000000;
            padding: 0 10%;
        }

        .hero h1 {
            font-size: clamp(3rem, 7vw, 6rem);
            font-weight: 700;
            line-height: 1.1;
        }

        /* Section Styles */
        section {
            padding: 100px 10%;
        }

        .white-bg {
            background: #ffffff;
            color: #000000;
        }

        .dark-bg {
            background: #1a1a1a;
            color: #ffffff;
        }

        .colored-bg {
            background: #f5f5f5;
            color: #000000;
        }

        .accent-bg {
            background: linear-gradient(135deg, #4a90e2 0%, #357abd 100%);
            color: #ffffff;
        }

        .pink-bg {
            background: #fef5f5;
            color: #000000;
        }

        /* Meta Bar */
        .meta-bar {
            background: #0a0a0a;
            color: #ffffff;
            padding: 50px 10%;
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 50px;
            text-align: center;
        }

        .meta-item h3 {
            font-size: 0.75rem;
            text-transform: uppercase;
            letter-spacing: 2px;
            color: #888;
            margin-bottom: 12px;
            font-weight: 600;
        }

        .meta-item p {
            font-size: 1.4rem;
            font-weight: 600;
            color: #FF6B6B;
        }

        /* Section Headers */
        .section-label {
            font-size: 0.8rem;
            text-transform: uppercase;
            letter-spacing: 2.5px;
            color: #e74c3c;
            margin-bottom: 20px;
            font-weight: 700;
        }

        .dark-bg .section-label {
            color: #FF6B6B;
        }

        .accent-bg .section-label {
            color: #ffffff;
            opacity: 0.9;
        }

        .colored-bg .section-label {
            color: #e74c3c;
        }

        .pink-bg .section-label {
            color: #e74c3c;
        }

        h2 {
            font-size: clamp(2.5rem, 5vw, 4rem);
            margin-bottom: 30px;
            font-weight: 700;
            line-height: 1.2;
        }

        h3 {
            font-size: 1.8rem;
            margin: 40px 0 20px 0;
            color: #FF6B6B;
        }

        .white-bg h3,
        .colored-bg h3,
        .pink-bg h3 {
            color: #e74c3c;
        }

        p {
            font-size: 1.2rem;
            line-height: 1.8;
            margin-bottom: 25px;
            opacity: 0.95;
        }

        ul {
            margin-left: 30px;
            margin-top: 20px;
        }

        li {
            font-size: 1.15rem;
            line-height: 1.9;
            margin-bottom: 15px;
            opacity: 0.9;
        }

        /* Quote Box */
        .quote-box {
            background: #ffffff;
            padding: 60px;
            border-radius: 20px;
            margin: 60px 0;
            box-shadow: 0 10px 40px rgba(0, 0, 0, 0.1);
        }

        .dark-bg .quote-box {
            background: #2a2a2a;
            box-shadow: none;
        }

        .accent-bg .quote-box {
            background: rgba(255, 255, 255, 0.15);
            box-shadow: none;
        }

        .quote-box p {
            font-size: 1.5rem;
            font-style: italic;
            line-height: 1.7;
            color: #333;
        }

        .dark-bg .quote-box p,
        .accent-bg .quote-box p {
            color: #ffffff;
        }

        /* Tags */
        .tags {
            display: flex;
            gap: 15px;
            flex-wrap: wrap;
            margin-top: 40px;
        }

        .tag {
            background: #f0f0f0;
            color: #333;
            padding: 12px 24px;
            border-radius: 30px;
            font-size: 1rem;
            font-weight: 500;
        }

        .dark-bg .tag {
            background: #333;
            color: #00d4ff;
        }

        .accent-bg .tag {
            background: rgba(255, 255, 255, 0.2);
            color: #ffffff;
        }

        /* Image Placeholder */
        .image-section {
            width: 100%;
            height: 500px;
            background: #f8f8f8;
            border-radius: 15px;
            display: flex;
            align-items: center;
            justify-content: center;
            margin: 60px 0;
            color: #999;
            font-size: 1.2rem;
        }

        .dark-bg .image-section {
            background: #2a2a2a;
            color: #666;
        }

        .accent-bg .image-section {
            background: rgba(255, 255, 255, 0.1);
            color: #ccc;
        }

        /* Grid Layouts */
        .feature-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 40px;
            margin-top: 50px;
        }

        .feature-card {
            padding: 40px;
            background: #f8f8f8;
            border-radius: 15px;
        }

        .dark-bg .feature-card {
            background: #2a2a2a;
        }

        .accent-bg .feature-card {
            background: rgba(255, 255, 255, 0.1);
        }

        .feature-card h3 {
            font-size: 1.5rem;
            margin-bottom: 15px;
            color: #e74c3c;
        }

        .dark-bg .feature-card h3 {
            color: #00d4ff;
        }

        .accent-bg .feature-card h3 {
            color: #ffffff;
        }

        .cta-button {
            display: inline-block;
            padding: 18px 40px;
            background: transparent;
            border: 2px solid #FF6B6B;
            color: #FF6B6B;
            text-decoration: none;
            border-radius: 50px;
            font-weight: 600;
            font-size: 1.1rem;
            transition: all 0.3s ease;
        }

        .cta-button:hover {
            background: #FF6B6B;
            color: #ffffff;
            transform: translateY(-3px);
        }

        @media (max-width: 968px) {
            section {
                padding: 60px 5%;
            }

            .meta-bar {
                grid-template-columns: 1fr;
                gap: 20px;
            }
        }
    </style>
</head>
<body>
    <a href="projects.html" class="back-btn">← Back to Projects</a>
    
    <!-- Hero -->
    <div class="hero">
        <h1>Señor Turtle: Search and Rescue Robotics</h1>
    </div>

    <!-- Meta Bar -->
    <div class="meta-bar">
        <div class="meta-item">
            <h3>ROLE</h3>
            <p>SLAM Engineer</p>
        </div>
        <div class="meta-item">
            <h3>CONTEXT</h3>
            <p>Mobile Robotics</p>
        </div>
        <div class="meta-item">
            <h3>DURATION</h3>
            <p>Jan 2024 - May 2024</p>
        </div>
        <div class="meta-item">
            <h3>TEAM SIZE</h3>
            <p>3 Members</p>
        </div>
    </div>

    <!-- Overview Section -->
    <section class="white-bg">
        <div class="section-label">PROJECT OVERVIEW</div>
        <h2>Autonomous search and rescue through advanced navigation</h2>
        <p>This project explores the integration of a modified Model Predictive Path Integral (m-MPPI) control algorithm into a robotic search and rescue operation using the Turtlebot3 platform. Utilizing the Turtlebot3 SLAM package, the robot autonomously maps an environment and identifies unexplored frontiers. These frontiers serve as goals for the m-MPPI control algorithm, which calculates optimal paths to navigate while continuing to map the surroundings.</p>
        <p>The objective is to enhance robotic capabilities in search and rescue missions through advanced navigation and mapping technologies. The Turtlebot3 platform is equipped with sensors and software that enable real-time localization and mapping of complex environments, a critical feature in rescue scenarios where precision and reliability are paramount.</p>
        <p>The integration of the m-MPPI control algorithm allows the robot to navigate efficiently by optimizing trajectory paths dynamically, considering both current and predictive state estimations. This project builds on foundational work in simultaneous localization and mapping (SLAM), specifically utilizing the Cartographer SLAM framework developed by Google, which supports robust 2D and 3D mapping crucial for diverse environments encountered during search and rescue operations.</p>
        
        <div class="tags">
            <span class="tag">ROS</span>
            <span class="tag">Cartographer SLAM</span>
            <span class="tag">Python</span>
            <span class="tag">Gazebo</span>
            <span class="tag">m-MPPI Control</span>
            <span class="tag">Computer Vision</span>
            <span class="tag">AprilTag Detection</span>
        </div>
        
        <!-- Skills Bars -->
        <div style="margin-top: 80px;">
            <h3 style="color: #e74c3c; margin-bottom: 40px;">Project Competencies</h3>
            
            <div style="margin-bottom: 30px;">
                <p style="font-size: 1rem; margin-bottom: 10px; font-weight: 500;">SLAM Implementation & Configuration</p>
                <div style="background: #e0e0e0; height: 8px; border-radius: 10px; overflow: hidden;">
                    <div style="background: linear-gradient(90deg, #e74c3c 0%, #c0392b 100%); width: 95%; height: 100%;"></div>
                </div>
            </div>
            
            <div style="margin-bottom: 30px;">
                <p style="font-size: 1rem; margin-bottom: 10px; font-weight: 500;">Autonomous Navigation & Path Planning</p>
                <div style="background: #e0e0e0; height: 8px; border-radius: 10px; overflow: hidden;">
                    <div style="background: linear-gradient(90deg, #e74c3c 0%, #c0392b 100%); width: 85%; height: 100%;"></div>
                </div>
            </div>
            
            <div style="margin-bottom: 30px;">
                <p style="font-size: 1rem; margin-bottom: 10px; font-weight: 500;">Sensor Fusion (LiDAR + IMU + Camera)</p>
                <div style="background: #e0e0e0; height: 8px; border-radius: 10px; overflow: hidden;">
                    <div style="background: linear-gradient(90deg, #e74c3c 0%, #c0392b 100%); width: 90%; height: 100%;"></div>
                </div>
            </div>
            
            <div style="margin-bottom: 30px;">
                <p style="font-size: 1rem; margin-bottom: 10px; font-weight: 500;">ROS System Architecture</p>
                <div style="background: #e0e0e0; height: 8px; border-radius: 10px; overflow: hidden;">
                    <div style="background: linear-gradient(90deg, #e74c3c 0%, #c0392b 100%); width: 88%; height: 100%;"></div>
                </div>
            </div>
            
            <div style="margin-bottom: 30px;">
                <p style="font-size: 1rem; margin-bottom: 10px; font-weight: 500;">Parameter Optimization & Tuning</p>
                <div style="background: #e0e0e0; height: 8px; border-radius: 10px; overflow: hidden;">
                    <div style="background: linear-gradient(90deg, #e74c3c 0%, #c0392b 100%); width: 92%; height: 100%;"></div>
                </div>
            </div>
            
            <div style="margin-bottom: 30px;">
                <p style="font-size: 1rem; margin-bottom: 10px; font-weight: 500;">Computer Vision & AprilTag Detection</p>
                <div style="background: #e0e0e0; height: 8px; border-radius: 10px; overflow: hidden;">
                    <div style="background: linear-gradient(90deg, #e74c3c 0%, #c0392b 100%); width: 80%; height: 100%;"></div>
                </div>
            </div>
            
            <div style="margin-bottom: 30px;">
                <p style="font-size: 1rem; margin-bottom: 10px; font-weight: 500;">Hardware Integration & Troubleshooting</p>
                <div style="background: #e0e0e0; height: 8px; border-radius: 10px; overflow: hidden;">
                    <div style="background: linear-gradient(90deg, #e74c3c 0%, #c0392b 100%); width: 78%; height: 100%;"></div>
                </div>
            </div>
        </div>
    </section>

    <!-- Media Placeholder -->
    <section class="colored-bg">
        <div class="section-label">PROJECT DEMONSTRATIONS</div>
        <h2>From challenges to success</h2>
        <p style="margin-bottom: 30px;">Watch Señor Turtle's evolution from early development challenges through SLAM validation to final successful navigation. This journey showcases the iterative nature of robotics development and the importance of rigorous parameter tuning.</p>
        
        <!-- Phase 1: Development Challenges -->
        <div style="margin: 60px 0; padding: 40px; background: rgba(231, 76, 60, 0.1); border-radius: 15px;">
            <h3 style="margin-bottom: 20px; margin-top: 0;">Phase 1: Development Process & Early Challenges</h3>
            <p style="margin-bottom: 40px;">The path to a robust search and rescue robot wasn't without obstacles. Initial attempts with aggressive parameters (full speed, 2π angular velocity range) produced entertaining but impractical results. These early tests revealed critical insights about the balance between exploration speed and safety.</p>
            
            <div style="max-width: 700px; margin: 0 auto;">
                <video controls style="width: 100%; border-radius: 15px; margin-bottom: 15px;">
                    <source src="media/video/House_Blooper1.webm" type="video/webm">
                    Your browser does not support the video tag.
                </video>
                <p style="font-size: 1rem; opacity: 0.8; text-align: center;">Early testing before optimal hyper-parameter tuning - revealing the need for reduced speed and tighter angular constraints</p>
            </div>
            
            <p style="margin-top: 30px; font-style: italic; opacity: 0.9;">These challenges drove the refinement process, leading to the optimized parameters: 50 trajectories, 10 lookahead states, reduced linear speed (0.07 m/s), and constrained angular range (2π/3).</p>
        </div>
        
        <!-- Phase 2: SLAM Validation -->
        <div style="margin: 60px 0; padding: 40px; background: rgba(74, 144, 226, 0.1); border-radius: 15px;">
            <h3 style="margin-bottom: 20px; margin-top: 0;">Phase 2: SLAM System Validation (TurtleBot World)</h3>
            <p style="margin-bottom: 40px;">With tuned parameters, we validated the Cartographer SLAM system's mapping capabilities in the TurtleBot World environment. These tests confirmed real-time map building, frontier detection, and the integration of LiDAR data for obstacle awareness.</p>
            
            <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 40px;">
                <div>
                    <video controls style="width: 100%; border-radius: 15px; margin-bottom: 15px;">
                        <source src="media/video/World_Blooper1.webm" type="video/webm">
                        Your browser does not support the video tag.
                    </video>
                    <p style="font-size: 1rem; opacity: 0.8; text-align: center;">Real-time SLAM mapping showing progressive map building as the robot explores the World environment</p>
                </div>
                <div>
                    <video controls style="width: 100%; border-radius: 15px; margin-bottom: 15px;">
                        <source src="media/video/World_Blooper2.webm" type="video/webm">
                        Your browser does not support the video tag.
                    </video>
                    <p style="font-size: 1rem; opacity: 0.8; text-align: center;">Frontier detection and path planning visualization demonstrating autonomous goal selection</p>
                </div>
            </div>
            
            <p style="margin-top: 30px; font-style: italic; opacity: 0.9;">The World environment provided an ideal testing ground for SLAM accuracy and loop closure detection, validating our sensor fusion approach before moving to more complex scenarios.</p>
        </div>
        
        <!-- Phase 3: Final Success -->
        <div style="margin: 60px 0; padding: 40px; background: rgba(255, 193, 7, 0.15); border-radius: 15px;">
            <h3 style="margin-bottom: 20px; margin-top: 0;">Phase 3: Complete System Performance (TurtleBot House)</h3>
            <p style="margin-bottom: 40px;">The culmination of development and tuning: Señor Turtle successfully navigates the complex House environment with furniture, narrow passages, and challenging obstacle configurations. The system demonstrates robust collision avoidance, smooth path execution, and reliable goal-reaching capabilities.</p>
            
            <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 40px;">
                <div>
                    <video controls style="width: 100%; border-radius: 15px; margin-bottom: 15px;">
                        <source src="media/video/House_Success1.webm" type="video/webm">
                        Your browser does not support the video tag.
                    </video>
                    <p style="font-size: 1rem; opacity: 0.8; text-align: center;">Confident navigation through complex indoor furniture layout with precise obstacle avoidance</p>
                </div>
                <div>
                    <video controls style="width: 100%; border-radius: 15px; margin-bottom: 15px;">
                        <source src="media/video/House_Success2.webm" type="video/webm">
                        Your browser does not support the video tag.
                    </video>
                    <p style="font-size: 1rem; opacity: 0.8; text-align: center;">m-MPPI controller successfully reaching goals while avoiding tables and furniture</p>
                </div>
                <div>
                    <video controls style="width: 100%; border-radius: 15px; margin-bottom: 15px;">
                        <source src="media/video/House_Success3.webm" type="video/webm">
                        Your browser does not support the video tag.
                    </video>
                    <p style="font-size: 1rem; opacity: 0.8; text-align: center;">Autonomous exploration with real-time map updates showing comprehensive coverage</p>
                </div>
                <div>
                    <video controls style="width: 100%; border-radius: 15px; margin-bottom: 15px;">
                        <source src="media/video/House_Success4.webm" type="video/webm">
                        Your browser does not support the video tag.
                    </video>
                    <p style="font-size: 1rem; opacity: 0.8; text-align: center;">Fully tuned system demonstrating smooth, collision-free operation ready for rescue missions</p>
                </div>
            </div>
            
            <p style="margin-top: 30px; font-style: italic; opacity: 0.9;">The House environment's complexity mirrors real-world rescue scenarios, validating that Señor Turtle can navigate debris-filled disaster sites safely and efficiently.</p>
        </div>
        
        <div style="text-align: center; margin-top: 50px; padding: 30px; background: white; border-radius: 15px;">
            <p style="font-size: 1.3rem; font-weight: 600; margin: 0;">This progression from challenges to validated success demonstrates the robustness of our integrated SLAM, planning, and control systems.</p>
        </div>
    </section>

    <!-- My Contribution -->
    <section class="dark-bg">
        <div class="section-label">MY INDIVIDUAL CONTRIBUTION</div>
        <h2>Setting up and validating SLAM infrastructure</h2>
        <p>I was responsible for setting up Cartographer SLAM in ROS and conducting comprehensive testing in simulation environments. This involved configuring the SLAM parameters for optimal performance in search and rescue scenarios, including tuning the loop closure detection, map update rates, and ensuring the system could handle the chaotic and unpredictable environments typical of disaster sites.</p>
        <p>My work focused on ensuring that the robot could create accurate, real-time maps of unknown environments—a critical foundation for autonomous exploration. I validated the SLAM system's performance across multiple Gazebo environments, including TurtleBot World and House scenarios, testing various configurations to ensure robust mapping capabilities under different conditions including debris-filled terrain and varying sensor conditions.</p>
        <p>Additionally, I assisted my teammates with the physical TurtleBot setup, helping troubleshoot hardware issues including battery replacement, LiDAR wire connector repairs, and system configuration. Although we faced challenges with the LiDAR driver that prevented real-world testing, the simulation results validated our SLAM implementation's effectiveness.</p>
    </section>

    <!-- SLAM System -->
    <section class="white-bg">
        <div class="section-label">SLAM SYSTEM</div>
        <h2>Cartographer SLAM for search and rescue</h2>
        <p>In this project, Cartographer SLAM technology plays a crucial role by enabling real-time simultaneous localization and mapping (SLAM) for the Turtlebot3 robotic platform. This system empowers the robot to navigate complex environments efficiently, enhancing its capability in search and rescue operations.</p>
        
        <h3>Relevance to Search and Rescue</h3>
        <p>The ability of Cartographer SLAM to create detailed 2D and 3D maps is essential for search and rescue missions, where rapid and accurate environmental assessment is crucial. The technology ensures that the robotic platform can navigate debris-filled or unfamiliar terrain by providing precise and up-to-date mapping information, facilitating effective search strategies and obstacle avoidance.</p>
        <p>In search and rescue missions, environments are often chaotic and unpredictable due to natural disasters or accidents. Cartographer SLAM enables robots like Señor Turtle to dynamically adjust their navigation strategies by creating accurate, real-time maps.</p>
        
        <h3>Functional Mechanism</h3>
        <p>Cartographer SLAM combines data from various sensors, including LiDARs and IMUs, to produce accurate maps that are critical for navigating and strategizing in dynamic or unpredictable environments. Its loop closure detection feature is particularly valuable, allowing the system to correct any drift in the map by recognizing previously visited locations, thereby maintaining the integrity and accuracy of the map throughout the mission.</p>
        <p>The LiDAR provides detailed 360-degree scans to detect obstacles and map the environment, while the IMU tracks the robot's orientation and acceleration, crucial for maintaining the correct posture and navigation accuracy under varying terrain conditions.</p>
        <p>The front end of the system, or <strong>Local SLAM</strong>, processes incoming sensor data to create local submaps. These submaps consist of small, manageable areas that are quickly processed to reflect the immediate surroundings of the robot. This step is essential for maintaining real-time responsiveness in dynamic environments where situational awareness is constantly changing.</p>
        <p>An integral component of the backend, <strong>Global SLAM</strong>, involves identifying places the robot has previously visited (loop closure). By recognizing these areas, Cartographer SLAM can adjust for cumulative mapping errors and drift, refining the overall map accuracy. The backend uses sophisticated algorithms to stitch submaps together, ensuring they align correctly based on detected loop closures.</p>
    </section>

    <!-- Challenge -->
    <section class="dark-bg">
        <div class="section-label">DESIGN CHALLENGE</div>
        <h2>Overcoming hardware and software obstacles</h2>
        <p>The primary technical challenge was enabling autonomous exploration in completely unknown environments typical of disaster scenarios. Unlike traditional navigation where maps are pre-loaded, our system needed to simultaneously build a map, localize itself within that map, and make intelligent decisions about which unexplored areas to investigate next—all in real-time with limited computational resources.</p>
        
        <h3>Hardware Challenges</h3>
        <p>Initially, the project's main goal was to test the developed system in the real world using a custom cardboard maze environment with AprilTag QR codes to represent victims. However, we faced significant hardware challenges when preparing Señor Turtle:</p>
        <ul>
            <li>Dead battery upon arrival</li>
            <li>Broken LiDAR wire connectors</li>
            <li>Missing key wiring from the setup</li>
            <li>Missing SD card (no installed OS or software)</li>
            <li>LiDAR driver issue causing the sensor to stop spinning after bring-up</li>
        </ul>
        <p>Despite extensive troubleshooting over several days, time constraints prevented us from resolving the LiDAR driver issue. Since the LiDAR spins properly when powered but not after bring-up, the most probable cause was the LiDAR driver. This forced us to validate our work exclusively through Gazebo simulations in TurtleBot World and House environments.</p>
    </section>

    <!-- Local Planning -->
    <section class="white-bg">
        <div class="section-label">LOCAL PLANNING & CONTROL</div>
        <h2>Frontier exploration and m-MPPI controller</h2>
        
        <h3>Frontier-Based Exploration</h3>
        <p>The system uses the map created by our SLAM packages to create goals based on unexplored "frontiers"—the boundaries between known and unknown areas of the map. The main goal of the algorithm is to group boundaries into a set of goals to explore, then choose the frontier that maximizes information gain.</p>
        <p>To keep computational cost low, this implementation makes use of the simplicity of Breadth-First-Search to comb through the updating map. The algorithm looks for areas that lie within 50-80 'steps' of the current pose (where a step is a translation in the map equal to the map resolution). Once those goals are computed, the algorithm chooses the goal closest to the robot and supplies that to a ROS topic.</p>
        
        <h3>Model Predictive Control (m-MPPI)</h3>
        <p>The system uses a modified version of Model Predictive Path Integral (m-MPPI) Control as its local path-planner. Given proper goals, the controller drives the TurtleBot safely through the environment, avoiding obstacles and reaching within a certain distance of the goal.</p>
        <p>Standard MPPI control is a stochastic trajectory optimization algorithm that uses a dynamic model of the system to predict future states based on randomized actions. The model costs each stored trajectory based on a custom cost function and uses weighted sampling to update the nominal trajectory.</p>
    </section>

    <!-- Technical Implementation -->
    <section class="colored-bg">
        <div class="section-label">TECHNICAL IMPLEMENTATION</div>
        <h2>m-MPPI process and optimization</h2>
        
        <h3>1. Computing Rollouts</h3>
        <p>The algorithm samples a number of random angular velocities between a specified range. Then, using Unicycle dynamics (a differential-drive model) created by Dr. Michael Everett for Northeastern's Mobile Robotics Course, trajectories are created following each angular velocity and a set linear velocity for a certain number of steps (lookahead states). Most of the stochasticity is removed from the original algorithm to cut computational costs, as angle sampling occurs only once each loop.</p>
        
        <h3>2. Costing Rollouts</h3>
        <p>The custom cost function is simplified to check 2 features of each state in each trajectory: occupancy and distance to goal. The first cost added is the Euclidean distance between a future state and the given goal.</p>
        <p>Initially, the SLAM map was used to check occupancy. The pose of each state was transformed to map row and column indices—an occupied state received a cost of 1000 and an unoccupied state received a cost of 1. However, due to the map being updated as the robot explores, there would generally be missing occupancy data causing several collisions.</p>
        <p>Therefore, the LiDAR scan was used instead. NumPy's norm and arctan2 functions calculate the angle and distance between the current robot's pose and future state pose. The LiDAR range corresponding to the closest angle determines whether the norm distance is within the acceptable distance of detected obstacles. The occupancy costs remained at 1 and 1000.</p>
        
        <h3>3. Cost Evaluation and Control Execution</h3>
        <p>m-MPPI uses weighted sampling to get the nominal trajectory. The probability of each trajectory is determined based on its cost. However, each loop, the next nominal action is replaced with the new nominal angular velocity rather than updated. Since each trajectory follows the same angular velocity and set linear velocity, the weighted sampling produces the nominal angular velocity to be used in the next action. Therefore, each action is independent of previous trajectories.</p>
    </section>

    <!-- Tuning -->
    <section class="dark-bg">
        <div class="section-label">PARAMETER OPTIMIZATION</div>
        <h2>Tuning hyper-parameters of m-MPPI</h2>
        <p>One of the many constants in life is the frustration of a robotics engineer while tuning hyper-parameters. One of the challenges faced in this project was tuning the controller to avoid obstacles. The hyper-parameters were tested in Gazebo environments (TurtleBot World and House).</p>
        <p>Initially, the TurtleBot was moving at full speed, sampling 30 trajectories, with an angular velocity sampling range of 2π. However, while that produced entertaining bloopers in simulation, it would have caused enough collisions to damage the TurtleBot in real life.</p>
        <p>Through rigorous trial and error, the following optimal hyper-parameters were chosen:</p>
        <ul>
            <li><strong>Number of trajectories to compute:</strong> 50</li>
            <li><strong>Number of states per trajectory:</strong> 10 lookahead states</li>
            <li><strong>Linear speed:</strong> 0.07 m/s (maximum is 0.22 m/s)</li>
            <li><strong>Angular velocity sampling range:</strong> 2π/3</li>
            <li><strong>Safety margin:</strong> 0.7m acceptable Euclidean distance between future state and LiDAR scan range to be considered unoccupied</li>
        </ul>
    </section>

    <!-- Vision System -->
    <section class="white-bg">
        <div class="section-label">VISION SYSTEM</div>
        <h2>AprilTag detection and camera-LiDAR fusion</h2>
        <p>Our main goal with the vision system was to detect victims (represented by AprilTags) and get accurate position estimates in the map. We utilized the apriltag_ros wrapper package by April Robotics to detect AprilTags and publish their positions and IDs over a /tag_detections topic.</p>
        
        <h3>The Challenge: Monocular Depth Estimation</h3>
        <p>The inherent issue with pose detection using a monocular camera (Raspberry Pi camera) is that it often gets the depth wrong, resulting in AprilTags appearing behind or in front of walls. This issue primarily arises from the projective transformation inherent in monocular vision systems, where image points are projected onto a 2D plane, causing distortions especially as the tag moves further away.</p>
        <p>With increasing distance, the tag's image resolution decreases, reducing the number of pixels that represent the tag. This lower pixel density adversely affects the ability of detection algorithms to accurately localize the tag's corners and edges, which are crucial for precise pose estimation. Additionally, lens distortions (radial and tangential) further exacerbate these errors.</p>
        <p>The pose error is inversely proportional to the square of the distance: <strong>Pose Error ∝ 1/Distance²</strong></p>
        
        <h3>Solution: Extrinsic Calibration</h3>
        <p>To address distortion challenges, we integrated pose data into the 2D LiDAR frame through extrinsic calibration. This approach leverages the accuracy and range capabilities of LiDAR systems to enhance the robustness of pose estimation, particularly in complex environments.</p>
        <p>The calibration process involved:</p>
        <ul>
            <li><strong>Manual point correspondence:</strong> 20 points were carefully selected in camera images corresponding to distinctive features identifiable in both image and LiDAR scan data</li>
            <li><strong>LiDAR association:</strong> Each image point was associated with corresponding 3D points from the LiDAR scan, treated as object points in 3D space</li>
            <li><strong>OpenCV's solvePnP:</strong> Computed the pose transformation using camera intrinsic parameters and established correspondences to derive rotation and translation vectors describing the transformation from the 3D object points' frame to the camera's coordinate frame</li>
            <li><strong>Rodrigues' rotation formula:</strong> Converted rotation vectors into full 3D rotation matrices for practical use in transformations</li>
        </ul>
        <p>Another technique we implemented recognizes previously identified tags and uses weighted averaging to update positions:</p>
        <p style="background: #f5f5f5; padding: 20px; border-radius: 10px; font-family: 'Courier New', monospace; font-size: 1rem;">
            if tag_id in self.closed_tags:<br>
            &nbsp;&nbsp;&nbsp;&nbsp;L = 0.95<br>
            &nbsp;&nbsp;&nbsp;&nbsp;self.closed_tags[tag_id] = (1-L) * TMA + L * self.closed_tags[tag_id]
        </p>
        <p>The more the system sees the same tag, the more accurate the position estimate becomes through this weighted averaging approach (L=0.95).</p>
    </section>

    <!-- Features Grid -->
    <section class="pink-bg">
        <div class="section-label">KEY FEATURES</div>
        <h2>System capabilities</h2>
        <div class="feature-grid">
            <div class="feature-card">
                <h3>Autonomous Exploration</h3>
                <p>Frontier-based BFS algorithm identifies unexplored regions within 50-80 map steps, maximizing information gain while maintaining low computational cost</p>
            </div>
            <div class="feature-card">
                <h3>Real-time SLAM</h3>
                <p>Cartographer creates detailed 2D maps with loop closure detection, correcting drift and maintaining map integrity throughout missions using LiDAR and IMU sensor fusion</p>
            </div>
            <div class="feature-card">
                <h3>Optimized Path Planning</h3>
                <p>Modified m-MPPI controller with 50 trajectories, 10 lookahead states, and custom cost function balancing obstacle avoidance with goal-reaching efficiency</p>
            </div>
            <div class="feature-card">
                <h3>Victim Detection</h3>
                <p>AprilTag recognition with camera-LiDAR extrinsic calibration and weighted averaging (0.95 factor) for improved accuracy over multiple detections</p>
            </div>
        </div>
    </section>

    <!-- Limitations -->
    <section class="dark-bg">
        <div class="section-label">LIMITATIONS & FUTURE WORK</div>
        <h2>Known constraints and next steps</h2>
        
        <h3>m-MPPI Limitations</h3>
        <p>As with all path-planning algorithms, m-MPPI comes with limitations. One common cause of failure is an unreachable goal. If the controller receives a goal that cannot be directly reached due to large obstacles or requires complex planning (like a maze environment), m-MPPI can get stuck.</p>
        <p>Therefore, the frontier exploration algorithm needs to be finely tuned to supply close goals that are directly reachable from the current state. Our BFS search algorithm does not go beyond 50-80 map steps, which limits the frontier exploration to shorter areas and can leave some regions unexplored.</p>
        <p>Another limitation is the LiDAR scan resolution. If a thin enough obstacle presents itself, the scan might not pick it up properly and the robot collides. One example is the table in the Gazebo House environment where initial runs caused collisions with table legs.</p>
        
        <h3>Proposed Improvements</h3>
        <ul>
            <li><strong>Additional local planner:</strong> Implement A* or Dijkstra's algorithm to create sub-goals along the way to farther frontiers. In this setup, frontier-based exploration would provide global goals (no matter how far out they are), and the local planner would provide local goals to the m-MPPI controller to reach those frontiers.</li>
            <li><strong>Higher resolution LiDAR:</strong> Upgrade to detect thinner obstacles more reliably and avoid collisions with small objects like table legs</li>
            <li><strong>Multi-camera systems:</strong> Utilize multiple cameras to capture different angles of AprilTags for more precise triangulation, reducing errors from angular and perspective distortions</li>
            <li><strong>Machine learning models:</strong> Train models to recognize and correct for AprilTag distortions, adapting dynamically to changes in distance and angle to improve pose estimation accuracy</li>
            <li><strong>Advanced edge detection:</strong> Employ sophisticated algorithms to mitigate effects of pixelation and blurring at greater distances</li>
        </ul>
    </section>

    <!-- Results -->
    <section class="accent-bg">
        <div class="section-label">RESULTS & IMPACT</div>
        <h2>Validated performance in simulation environments</h2>
        <p>Testing for the search and rescue algorithms occurred in two Gazebo environments: TurtleBot World and TurtleBot House. Since the Frontier Detection Planner was not fully developed at time of testing, the m-MPPI planner was fed goals manually in a queue. In these tests, the TurtleBot SLAM package with gmapping was used instead of Cartographer SLAM due to computational costs. However, since m-MPPI was modified to only use LiDAR scans for collisions, Cartographer SLAM was not necessary for these tests.</p>
        <p>The results demonstrate that this modified MPPI implementation can work properly as a local planner at a significantly lower computational cost than standard MPPI. The system successfully navigated both environments, avoiding obstacles and reaching goals autonomously.</p>
        <p>The maps and navigation paths generated by Cartographer SLAM can be shared in real-time with rescue coordinators and other automated systems, facilitating coordinated response where multiple units are involved. The detailed and updated 3D maps allow rescue teams to make informed decisions about where to focus their efforts, how to allocate resources, and when to deploy human personnel safely.</p>
        
        <div class="quote-box">
            <p>"This project proved pivotal in reducing human intervention and increasing efficiency in emergency scenarios, demonstrating the potential for robotic systems to save lives in dangerous search and rescue operations."</p>
        </div>
    </section>

    <!-- SLAM Deep Dive -->
    <section class="white-bg">
        <div class="section-label">SLAM SYSTEM ARCHITECTURE</div>
        <h2>How Cartographer SLAM powers autonomous mapping</h2>
        <p>Cartographer SLAM harnesses data from multiple sensors equipped on the Turtlebot3, primarily LiDAR and IMU. The LiDAR provides detailed 360-degree scans to detect obstacles and map the environment, while the IMU tracks the robot's orientation and acceleration.</p>
        
        <h3>Local SLAM (Frontend)</h3>
        <p>Processes real-time data from sensors to create submaps that represent the immediate environment. It ensures quick adaptation to changes, critical in dynamic rescue scenes where debris and obstacles may shift unexpectedly. Each submap is a locally consistent representation of the environment. As Señor Turtle moves through search areas, new submaps are continuously generated with overlapping coverage to ensure comprehensive map integrity.</p>
        
        <h3>Global SLAM (Backend)</h3>
        <p>Works to merge submaps into a comprehensive and coherent global map. It performs loop closure detection, which is crucial for correcting any drift or misalignment in the map due to the robot's movement over time. The backend uses sophisticated algorithms to stitch submaps together, ensuring they align correctly based on detected loop closures and the overall geometry of the mapped area.</p>
        
        <h3>Configuration for Rescue Missions</h3>
        <ul>
            <li>System configured to process data at higher rates to ensure environmental changes are quickly reflected in the map—essential for navigating safely through unstable structures</li>
            <li>Configurations optimized to ensure accurate sensor data integration under challenging conditions (varying lighting, physical obstructions)</li>
            <li>Routes and navigation plans update dynamically in response to new SLAM data, enabling instant path adaptation to avoid newly detected obstacles</li>
            <li>Paths calculated with safety margins considering robot dimensions and typical debris sizes in rescue sites</li>
            <li>Loop closure algorithms enhanced to recognize previously mapped areas quickly, even under partial visibility or when significant areas have been altered</li>
            <li>Robust error handling protocols manage sensor malfunctions or data inconsistencies, allowing the system to revert to safe states autonomously</li>
        </ul>
    </section>

    <!-- Team Contributions -->
    <section class="dark-bg">
        <div class="section-label">TEAM CONTRIBUTIONS</div>
        <h2>Collaborative development</h2>
        <p>This project was a collaborative effort among three team members, with clearly defined responsibilities:</p>
        <ul>
            <li><strong>Febin Wilson (Me):</strong> Setup Cartographer SLAM in ROS and tested it extensively on simulations, configured SLAM parameters for optimal search and rescue performance, validated system across multiple Gazebo environments</li>
            <li><strong>Adnan Amir:</strong> Setup and tested Vision System (AprilTag Detection), developed the Frontier Detection Algorithm for autonomous exploration, helped with TurtleBot hardware setup and troubleshooting</li>
            <li><strong>Ramez Mubarak:</strong> Setup TurtleBot physical system and OS/Software Configuration, developed and tested the complete m-MPPI ROS package from scratch, implemented the custom cost function and trajectory optimization</li>
        </ul>
        <p>All team members contributed to the presentation and report by writing up and presenting their respective work, while assisting others on their tasks as needed. The collaborative nature of this project was essential to integrating the SLAM, planning, and vision systems into a cohesive search and rescue platform.</p>
    </section>

    <!-- Skills -->
    <section class="white-bg">
        <div class="section-label">SKILLS DEMONSTRATED</div>
        <h2>Technical expertise applied</h2>
        
        <!-- Skills Bars -->
        <div style="margin: 50px 0 60px 0;">
            <div style="margin-bottom: 35px;">
                <p style="font-size: 1.1rem; margin-bottom: 12px; font-weight: 600; color: #333;">Robot Operating System (ROS)</p>
                <div style="background: #e8e8e8; height: 10px; border-radius: 10px; overflow: hidden;">
                    <div style="background: linear-gradient(90deg, #e74c3c 0%, #c0392b 100%); width: 95%; height: 100%; border-radius: 10px;"></div>
                </div>
            </div>
            
            <div style="margin-bottom: 35px;">
                <p style="font-size: 1.1rem; margin-bottom: 12px; font-weight: 600; color: #333;">Cartographer SLAM & Localization</p>
                <div style="background: #e8e8e8; height: 10px; border-radius: 10px; overflow: hidden;">
                    <div style="background: linear-gradient(90deg, #e74c3c 0%, #c0392b 100%); width: 93%; height: 100%; border-radius: 10px;"></div>
                </div>
            </div>
            
            <div style="margin-bottom: 35px;">
                <p style="font-size: 1.1rem; margin-bottom: 12px; font-weight: 600; color: #333;">Python Programming & Algorithm Development</p>
                <div style="background: #e8e8e8; height: 10px; border-radius: 10px; overflow: hidden;">
                    <div style="background: linear-gradient(90deg, #e74c3c 0%, #c0392b 100%); width: 90%; height: 100%; border-radius: 10px;"></div>
                </div>
            </div>
            
            <div style="margin-bottom: 35px;">
                <p style="font-size: 1.1rem; margin-bottom: 12px; font-weight: 600; color: #333;">Sensor Fusion (LiDAR-Camera Integration)</p>
                <div style="background: #e8e8e8; height: 10px; border-radius: 10px; overflow: hidden;">
                    <div style="background: linear-gradient(90deg, #e74c3c 0%, #c0392b 100%); width: 88%; height: 100%; border-radius: 10px;"></div>
                </div>
            </div>
            
            <div style="margin-bottom: 35px;">
                <p style="font-size: 1.1rem; margin-bottom: 12px; font-weight: 600; color: #333;">Computer Vision (OpenCV, AprilTag Detection)</p>
                <div style="background: #e8e8e8; height: 10px; border-radius: 10px; overflow: hidden;">
                    <div style="background: linear-gradient(90deg, #e74c3c 0%, #c0392b 100%); width: 82%; height: 100%; border-radius: 10px;"></div>
                </div>
            </div>
            
            <div style="margin-bottom: 35px;">
                <p style="font-size: 1.1rem; margin-bottom: 12px; font-weight: 600; color: #333;">Gazebo Simulation & Testing</p>
                <div style="background: #e8e8e8; height: 10px; border-radius: 10px; overflow: hidden;">
                    <div style="background: linear-gradient(90deg, #e74c3c 0%, #c0392b 100%); width: 85%; height: 100%; border-radius: 10px;"></div>
                </div>
            </div>
            
            <div style="margin-bottom: 35px;">
                <p style="font-size: 1.1rem; margin-bottom: 12px; font-weight: 600; color: #333;">System Integration & Debugging</p>
                <div style="background: #e8e8e8; height: 10px; border-radius: 10px; overflow: hidden;">
                    <div style="background: linear-gradient(90deg, #e74c3c 0%, #c0392b 100%); width: 87%; height: 100%; border-radius: 10px;"></div>
                </div>
            </div>
            
            <div style="margin-bottom: 35px;">
                <p style="font-size: 1.1rem; margin-bottom: 12px; font-weight: 600; color: #333;">Hardware Troubleshooting</p>
                <div style="background: #e8e8e8; height: 10px; border-radius: 10px; overflow: hidden;">
                    <div style="background: linear-gradient(90deg, #e74c3c 0%, #c0392b 100%); width: 78%; height: 100%; border-radius: 10px;"></div>
                </div>
            </div>
            
            <div style="margin-bottom: 35px;">
                <p style="font-size: 1.1rem; margin-bottom: 12px; font-weight: 600; color: #333;">Team Collaboration & Documentation</p>
                <div style="background: #e8e8e8; height: 10px; border-radius: 10px; overflow: hidden;">
                    <div style="background: linear-gradient(90deg, #e74c3c 0%, #c0392b 100%); width: 92%; height: 100%; border-radius: 10px;"></div>
                </div>
            </div>
        </div>
        
        <div class="tags">
            <span class="tag">Robot Operating System (ROS)</span>
            <span class="tag">Cartographer SLAM</span>
            <span class="tag">Computer Vision (OpenCV)</span>
            <span class="tag">Python Programming</span>
            <span class="tag">Algorithm Development (BFS, m-MPPI)</span>
            <span class="tag">Sensor Fusion (LiDAR-Camera)</span>
            <span class="tag">Gazebo Simulation</span>
            <span class="tag">System Integration</span>
            <span class="tag">Hardware Troubleshooting</span>
            <span class="tag">Research & Documentation</span>
            <span class="tag">Team Collaboration</span>
            <span class="tag">AprilTag Detection</span>
        </div>
    </section>

    <!-- GitHub Link -->
    <section class="colored-bg" style="text-align: center;">
        <h2 style="margin-bottom: 40px;">View the complete project</h2>
        <div style="display: flex; gap: 30px; justify-content: center; align-items: center; flex-wrap: wrap;">
            <a href="https://github.com/RMubarak/Mobile-Robotics-Project" target="_blank" class="cta-button">
                View Project on GitHub →
            </a>
            <a href="media/doc/senor-turtle-report.pdf" download class="cta-button" style="background: #FF6B6B; color: white; border-color: #FF6B6B;">
                Download Project Report (PDF) ↓
            </a>
        </div>
        <p style="margin-top: 30px; font-size: 1rem; opacity: 0.7;">Includes code, documentation, demo videos, and comprehensive technical report</p>
    </section>

    <script>
        // No theme toggle - case study style
    </script>
</body>
</html>
